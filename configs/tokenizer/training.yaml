type: 'tokenizer'
commitment_loss_weight: 1.5
random_seed: 42
batch_size: 128
num_epochs: 130
warmup_initial_learning_rate: 1e-6
warmup_final_learning_rate: 1e-4
warmup_epochs: 10
optimizer:
  name: AdamW
  params:
    lr: ${training.warmup_initial_learning_rate}
    weight_decay: 0.001
lr_scheduler:
  main:
    name: CosineAnnealingLR
    params:
      eta_min: 1e-6
  warmup:
    name: LinearWarmupScheduler
    params: {}
criterion:
  name: MSELoss